
<div align="center">
<h1>ADAPT: Anchored Multimodal Physiological Transformer</h1>
<a href="https://www.python.org/"><img alt="PyTorch" src="https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=fff"></a>
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra 1.3" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
</div>

Inspired by ADAPT [[Paper]](https://openreview.net/pdf?id=WDZg4P97gr) and OmniSat [[Paper]](https://arxiv.org/pdf/2404.08351)

<img width="1049" alt="Screenshot 2024-02-07 at 11 02 58" src="pic/architecture.jpg">


# Project Structure
```
â”œâ”€â”€ configs                      Hydra configs
â”‚   â”œâ”€â”€ machine                  Machine configs (gpu, ...)
â”‚   â”œâ”€â”€ model                    Model cfg
â”‚   â”œâ”€â”€ multimodal               Data configs
â”‚   â”œâ”€â”€ paths                    Project paths configs
â”‚   â”œâ”€â”€ config.yaml              Main config for training
â”‚   â”œâ”€â”€ PT_ADAPT.yaml            Main config for training on PastisHD
â”‚   â”œâ”€â”€ TS_ADAPT.yaml            Main config for training on TreeSAT-AI
â”‚   â””â”€â”€ video-extract.yaml       Config for video feature extraction 
â”‚
â”œâ”€â”€ src                    
â”‚   â”œâ”€â”€ datamodule                Data
â”‚   â”‚   â”œâ”€â”€ datasets             
â”‚   â”‚   â””â”€â”€ multimodal_datamodule.py        
â”‚   â”‚
â”‚   â”œâ”€â”€ models   
|   |   â”œâ”€â”€ modules               Modules used in the model
|   |   â”œâ”€â”€ encoders              Encoders for earth observation data
|   |   â”œâ”€â”€ losses                Loss functions for downstream tasks
|   |   â””â”€â”€ adapt.py              ADAPT model       
â”‚   â”‚     
â”‚   â””â”€â”€ utils   
â”‚       â”œâ”€â”€ evaluation          
|       â”œâ”€â”€ preprocessing     
|       â””â”€â”€ training                  
â”‚
â”œâ”€â”€ .gitignore                   List of files ignored by git
â”œâ”€â”€ requirements.txt             File for installing python dependencies
â”œâ”€â”€ train.py                     Main script for training
â”œâ”€â”€ License                      
â””â”€â”€ README.md
```

# ðŸš€ Quickstart
## Set-Up the environment
- Install Anaconda or MiniConda
- Run `conda create -n multi python=3.9`
- Activate multi: `conda activate multi`
- Install pytorch 1.12 and torchvision 0.13 that match your device
    - For GPU: 
    `conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.6 -c pytorch -c conda-forge`
- Dependencies apart from `pytorch` may be install with the `pip isntall -r requirements.txt`.

## Prepare the data
- For the dataset, $\texttt{TreeSAT-AI}$ information and data request can be found [here](https://huggingface.co/datasets/IGNF/TreeSatAI-Time-Series).
- Put the data in a directory called "misc/TreeSat"
- Change the path in the cfg/paths/directories.yaml



## Train ADAPT

The code is adapted to wandb logger, if you wish to use a logger make sure to be logged in to wandb before starting.
To run the code on TreeSAT-AI:

```bash
python train_TS.py
```

To disable the logger:
```bash
python train_TS.py log=False
```
