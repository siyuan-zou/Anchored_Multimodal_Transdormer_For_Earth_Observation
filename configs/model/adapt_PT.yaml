modalities: ${modalities}
encoders: 
  type: ["PASTIS"]

  aerial:
    patch_size: 40
    in_chans: 3
    embed_dim: 256
    bias: False
    res: True
    gp_norm: 4
  s2:
    in_channels: 10
    n_head: 16
    d_k: 8
    mlp:
      - 256
      - 512
      - 256
    mlp_in:
      - 32
      - 128
      - 256
    dropout: 0.2
    T: 734
    in_norm: True
    positional_encoding: True
    patch_size: 4
  s1-asc:
    in_channels: 3
    n_head: 16
    d_k: 8
    mlp:
      - 256
      - 512
      - 256
    mlp_in:
      - 32
      - 128
      - 256
    dropout: 0.2
    T: 734
    in_norm: False
    positional_encoding: True
    patch_size: 4

  projection: True
  projection_method: ${projection_method}
  embed_dim: 256
  pooling_method: ${pooling_method}

num_patches: 1024
embed_dim: 256
depth: 6
num_heads: 16
mlp_ratio: 4.
class_token: True
drop_rate: 0.2
pos_drop_rate: 0.2
patch_drop_rate: 0.0
drop_path_rate: 0.2
attn_drop_rate: 0.2

anchor: ${anchor}

transformer: 
  _target_: src.models.modules.attention.Attention
  d_model: 64
  dropout: 0.2
  n_heads: 4
  n_blocks: 1

supervised_loss:
  max_epochs: ${max_epochs}
  lr: 1e-4
  weight_decay: 0.05
  warmup: 4
  min_lr: 1e-8
  sch: True

contrastive_loss: 
  max_epochs: ${max_epochs}
  lr: 1e-4
  weight_decay: 0.0001
  warmup: 4
  min_lr: 1e-8
  learnable_scale: True
  sch: True
  modality_dropout: True
  temperature: 0.07
  temperature_max: 1
  temperature_min: 0.07
  ts_augment: True
  cos: false
  gamma: 0.05
  noise_sigma: 0.1

anchoring_loss: 
  max_epochs: ${max_epochs}
  lr: 0.001
  weight_decay: 0.0001
  cos: True
  learnable_scale: False
  warmup: 4
  min_lr: 1e-8
  sch: True
  temperature_max: 1
  temperature_min: 0.07
  period: 100
  gamma: 0.05